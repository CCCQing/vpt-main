# Visual Prompt Tuning 

https://arxiv.org/abs/2203.12119 

------

This repository contains the official PyTorch implementation for Visual Prompt Tuning.

![vpt_teaser](https://github.com/KMnP/vpt/blob/main/imgs/teaser.png)

## Environment settings

See `env_setup.sh`

## Structure of the this repo (key files are marked with ğŸ‘‰):

- `src/configs`: handles config parameters for the experiments.
                               å®éªŒçš„ä¸»è¦é…ç½®è®¾ç½®åŠå…¶æ¯ä¸ªé…ç½®çš„è§£é‡Šã€‚
  * ğŸ‘‰ `src/config/config.py`: <u>main config setups for experiments and explanation for each of them. </u> 
              åŠ è½½å’Œè®¾ç½®è¾“å…¥æ•°æ®é›†ã€‚è¿™äº›å†…å®¹src/data/vtab_datasetså€Ÿé‰´è‡ª  
- `src/data`: loading and setup input datasets. The `src/data/vtab_datasets` are borrowed from 

  [VTAB github repo](https://github.com/google-research/task_adaptation/tree/master/task_adaptation/data).

                ä¸»è¦çš„è®­ç»ƒå’Œè¯„ä¼°æ´»åŠ¨åœ¨è¿™é‡Œã€‚
- `src/engine`: main training and eval actions here.
                å¤„ç†ä¸åŒå¾®è°ƒåè®®çš„ä¸»å¹²æ‹±é—¨å’Œå¤´éƒ¨
- `src/models`: handles backbone archs and heads for different fine-tuning protocols 
                                ä¸€ä¸ªæ–‡ä»¶å¤¹åŒ…å«vit_backbonesä¸ VPT æŒ‡å®šçš„æ–‡ä»¶å¤¹ä¸­ç›¸åŒçš„éª¨å¹²æ–‡ä»¶ã€‚æ­¤æ–‡ä»¶å¤¹åº”åŒ…å«ä¸ vit_backbones
    * ğŸ‘‰`src/models/vit_prompt`: <u>a folder contains the same backbones in `vit_backbones` folder,</u> specified for VPT. This folder should contain the same file names as those in  `vit_backbones`
                                    åŸºäº Transformer çš„æ¨¡å‹çš„ä¸»æ¨¡å‹ â—ï¸æ³¨æ„â—ï¸ï¼šå½“å‰ç‰ˆæœ¬ä»…æ”¯æŒ ViTã€Swin ä»¥åŠå¸¦æœ‰ maeã€moco-v3 çš„ ViT
    * ğŸ‘‰ `src/models/vit_models.py`: <u>main model for transformer-based models</u> â—ï¸Noteâ—ï¸: Current version only support ViT, Swin and ViT with mae, moco-v3
                                    è¿™é‡Œçš„ä¸»è¦æ“ä½œæ˜¯åˆ©ç”¨é…ç½®å¹¶æ„å»ºæ¨¡å‹æ¥è®­ç»ƒ/è¯„ä¼°ã€‚
    * `src/models/build_model.py`: main action here to utilize the config and build the model to train / eval.

- `src/solver`: optimization, losses and learning rate schedules.  ä¼˜åŒ–ã€æŸå¤±å’Œå­¦ä¹ ç‡è®¡åˆ’ã€‚
- `src/utils`: helper functions for io, loggings, training, visualizations. ç”¨äº ioã€æ—¥å¿—ã€è®­ç»ƒã€å¯è§†åŒ–çš„è¾…åŠ©å‡½æ•°ã€‚
- ğŸ‘‰`train.py`: call this one for training and eval a model with a specified transfer type. è°ƒç”¨è¿™ä¸ªæ¥è®­ç»ƒå¹¶è¯„ä¼°å…·æœ‰æŒ‡å®šä¼ è¾“ç±»å‹çš„æ¨¡å‹ã€‚
  -                 è°ƒç”¨æ­¤è„šæœ¬æ¥è°ƒæ•´å…·æœ‰æŒ‡å®šè¿ç§»ç±»å‹çš„æ¨¡å‹çš„å­¦ä¹ ç‡å’Œæƒé‡è¡°å‡ã€‚æˆ‘ä»¬å°†æ­¤è„šæœ¬ç”¨äº FGVC ä»»åŠ¡ã€‚
- ğŸ‘‰`tune_fgvc.py`: call this one for tuning learning rate and weight decay for a model with a specified transfer type. We used this script for FGVC tasks.
  -                  è°ƒç”¨æ­¤æ–¹æ³•è°ƒæ•´ vtab ä»»åŠ¡ï¼šä½¿ç”¨ 800/200 åˆ†å‰²æ¥æ‰¾åˆ°æœ€ä½³ lr å’Œ wdï¼Œå¹¶ä½¿ç”¨æœ€ä½³ lr/wd è¿›è¡Œæœ€ç»ˆè¿è¡Œ
- ğŸ‘‰`tune_vtab.py`: call this one for tuning vtab tasks: use 800/200 split to find the best lr and wd, and use the best lr/wd for the final runs
- `launch.py`: contains functions used to launch the job. åŒ…å«ç”¨äºå¯åŠ¨ä½œä¸šçš„åŠŸèƒ½ã€‚

## Experiments

### Key configs:

- ğŸ”¥VPT related:
  - MODEL.PROMPT.NUM_TOKENS: prompt length æç¤ºé•¿åº¦
  - MODEL.PROMPT.DEEP: deep or shallow prompt æ·±æç¤ºæˆ–æµ…æç¤º
- Fine-tuning method specification: å¾®è°ƒæ–¹æ³•è¯´æ˜
  - MODEL.TRANSFER_TYPE æ¨¡å‹.ä¼ è¾“ç±»å‹
- Vision backbones:
  - DATA.FEATURE: specify which representation to use æŒ‡å®šè¦ä½¿ç”¨çš„è¡¨ç¤ºå½¢å¼
  - MODEL.TYPE: the general backbone type, e.g., "vit" or "swin" é€šç”¨éª¨å¹²ç±»å‹ï¼Œä¾‹å¦‚â€œvitâ€æˆ–â€œswinâ€
  - MODEL.MODEL_ROOT: folder with pre-trained model checkpoints åŒ…å«é¢„è®­ç»ƒæ¨¡å‹æ£€æŸ¥ç‚¹çš„æ–‡ä»¶å¤¹
- Optimization related: ä¼˜åŒ–ç›¸å…³
  - SOLVER.BASE_LR: learning rate for the experiment å®éªŒçš„å­¦ä¹ ç‡
  - SOLVER.WEIGHT_DECAY: weight decay value for the experiment å®éªŒçš„æƒé‡è¡°å‡å€¼
  - DATA.BATCH_SIZE æ•°æ®.æ‰¹é‡å¤§å°
- Datasets related:
  - DATA.NAME æ•°æ®åç§°
  - DATA.DATAPATH: where you put the datasets æ”¾ç½®æ•°æ®é›†çš„ä½ç½®
  - DATA.NUMBER_CLASSES
- Others: 
  - RUN_N_TIMES: ensure only run once in case for duplicated submision, not used during vtab runs ç¡®ä¿åªè¿è¡Œä¸€æ¬¡ï¼Œä»¥é˜²é‡å¤æäº¤ï¼Œåœ¨ vtab è¿è¡ŒæœŸé—´ä¸ä½¿ç”¨
  - OUTPUT_DIR: output dir of the final model and logs æœ€ç»ˆæ¨¡å‹å’Œæ—¥å¿—çš„è¾“å‡ºç›®å½•
  - MODEL.SAVE_CKPT: if set to `True`, will save model ckpts and final output of both val and test set å¦‚æœè®¾ç½®ä¸ºTrueï¼Œå°†ä¿å­˜æ¨¡å‹ ckpts ä»¥åŠ val å’Œæµ‹è¯•é›†çš„æœ€ç»ˆè¾“å‡º

### Datasets preperation:

See Table 8 in the Appendix for dataset details. 

- Fine-Grained Visual Classification tasks (FGVC): The datasets can be downloaded following the official links. We split the training data if the public validation set is not available. The splitted dataset can be found here: [Dropbox](https://cornell.box.com/v/vptfgvcsplits), [Google Drive](https://drive.google.com/drive/folders/1mnvxTkYxmOr2W9QjcgS64UBpoJ4UmKaM?usp=sharing). 

  - [CUB200 2011](https://data.caltech.edu/records/65de6-vp158)

  - [NABirds](http://info.allaboutbirds.org/nabirds/)

  - [Oxford Flowers](https://www.robots.ox.ac.uk/~vgg/data/flowers/)

  - [Stanford Dogs](http://vision.stanford.edu/aditya86/ImageNetDogs/main.html)

  - [Stanford Cars](https://ai.stanford.edu/~jkrause/cars/car_dataset.html)
è§†è§‰ä»»åŠ¡é€‚åº”åŸºå‡†ï¼ˆVTABï¼‰ï¼šè¯·å‚é˜…VTAB_SETUP.mdè¯¦ç»†è¯´æ˜å’Œæç¤ºã€‚
- [Visual Task Adaptation Benchmark](https://google-research.github.io/task_adaptation/) (VTAB): see [`VTAB_SETUP.md`](https://github.com/KMnP/vpt/blob/main/VTAB_SETUP.md) for detailed instructions and tips.

### Pre-trained model preperation é¢„è®­ç»ƒæ¨¡å‹å‡†å¤‡

Download and place the pre-trained Transformer-based backbones to `MODEL.MODEL_ROOT` (ConvNeXt-Base and ResNet50 would be automatically downloaded via the links in the code). Note that you also need to rename the downloaded ViT-B/16 ckpt from `ViT-B_16.npz` to `imagenet21k_ViT-B_16.npz`.

See Table 9 in the Appendix for more details about pre-trained backbones.
ä¸‹è½½å¹¶æ”¾ç½®é¢„å…ˆè®­ç»ƒå¥½çš„åŸºäº Transformer çš„ä¸»å¹²ç½‘ç»œåˆ°MODEL.MODEL_ROOTï¼ˆConvNeXt-Base å’Œ ResNet50 å°†é€šè¿‡ä»£ç ä¸­çš„é“¾æ¥è‡ªåŠ¨ä¸‹è½½ï¼‰ã€‚è¯·æ³¨æ„ï¼Œæ‚¨è¿˜éœ€è¦å°†ä¸‹è½½çš„ ViT-B/16 ckpt ä» é‡å‘½åViT-B_16.npzä¸ºimagenet21k_ViT-B_16.npzã€‚

<table><tbody>
<!-- START TABLE -->
<!-- TABLE HEADER -->
<th valign="bottom">Pre-trained Backbone</th>
<th valign="bottom">Pre-trained Objective</th>
<th valign="bottom">Link</th>
<th valign="bottom">md5sum</th>
<!-- TABLE BODY -->
<tr><td align="left">ViT-B/16</td>
<td align="center">Supervised</td>
<td align="center"><a href="https://storage.googleapis.com/vit_models/imagenet21k/ViT-B_16.npz">link</a></td>
<td align="center"><tt>d9715d</tt></td>
</tr>
<tr><td align="left">ViT-B/16</td>
<td align="center">MoCo v3</td>
<td align="center"><a href="https://dl.fbaipublicfiles.com/moco-v3/vit-b-300ep/linear-vit-b-300ep.pth.tar">link</a></td>
<td align="center"><tt>8f39ce</tt></td>
</tr>
<tr><td align="left">ViT-B/16</td>
<td align="center">MAE</td>
<td align="center"><a href="https://dl.fbaipublicfiles.com/mae/pretrain/mae_pretrain_vit_base.pth">link</a></td>
<td align="center"><tt>8cad7c</tt></td>
</tr>
<tr><td align="left">Swin-B</td>
<td align="center">Supervised</td>
<td align="center"><a href="https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_base_patch4_window7_224_22k.pth">link</a></td>
<td align="center"><tt>bf9cc1</tt></td>
</tr>
<tr><td align="left">ConvNeXt-Base</td>
<td align="center">Supervised</td>
<td align="center"><a href="https://dl.fbaipublicfiles.com/convnext/convnext_base_22k_224.pth">link</a></td>
<td align="center"><tt>-</tt></td>
</tr>
<tr><td align="left">ResNet-50</td>
<td align="center">Supervised</td>
<td align="center"><a href="https://pytorch.org/vision/stable/models.html">link</a></td>
<td align="center"><tt>-</tt></td>
</tr>
</tbody></table>

### Examples for training and aggregating results è®­ç»ƒå’Œæ±‡æ€»ç»“æœçš„ç¤ºä¾‹
è¯·å‚é˜…demo.ipynbå¦‚ä½•ä½¿ç”¨è¿™ä¸ª repoã€‚
See [`demo.ipynb`](https://github.com/KMnP/vpt/blob/main/demo.ipynb) for how to use this repo.

### Hyperparameters for experiments in paper è®ºæ–‡ä¸­å®éªŒçš„è¶…å‚æ•°
è¡¨ 1-2ã€å›¾ 3-4ã€è¡¨ 4-5 ä¸­ä½¿ç”¨çš„è¶…å‚æ•°å€¼ï¼ˆVPT çš„æç¤ºé•¿åº¦/é€‚é…å™¨çš„å‡å°‘ç‡ã€åŸºç¡€å­¦ä¹ ç‡ã€æƒé‡è¡°å‡å€¼ï¼‰å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°
The hyperparameter values used (prompt length for VPT / reduction rate for Adapters, base learning rate, weight decay values) in Table 1-2, Fig. 3-4, Table 4-5 can be found here: [Dropbox](https://cornell.box.com/s/lv10kptgyrm8uxb6v6ctugrhao24rs2z) / [Google Drive](https://drive.google.com/drive/folders/1ldhqkXelHDXq4bG7qpKn5YEfU6sRehJH?usp=sharing). 

## Citation

If you find our work helpful in your research, please cite it as:

```
@inproceedings{jia2022vpt,
  title={Visual Prompt Tuning},
  author={Jia, Menglin and Tang, Luming and Chen, Bor-Chun and Cardie, Claire and Belongie, Serge and Hariharan, Bharath and Lim, Ser-Nam},
  booktitle={European Conference on Computer Vision (ECCV)},
  year={2022}
}
```

## License

The majority of VPT is licensed under the CC-BY-NC 4.0 license (see [LICENSE](https://github.com/KMnP/vpt/blob/main/LICENSE) for details). Portions of the project are available under separate license terms: GitHub - [google-research/task_adaptation](https://github.com/google-research/task_adaptation) and [huggingface/transformers](https://github.com/huggingface/transformers) are licensed under the Apache 2.0 license; [Swin-Transformer](https://github.com/microsoft/Swin-Transformer), [ConvNeXt](https://github.com/facebookresearch/ConvNeXt) and [ViT-pytorch](https://github.com/jeonsworld/ViT-pytorch) are licensed under the MIT license; and [MoCo-v3](https://github.com/facebookresearch/moco-v3) and [MAE](https://github.com/facebookresearch/mae) are licensed under the Attribution-NonCommercial 4.0 International license.
